{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "_ai32xyJE4lJ"
      },
      "source": [
        "# Bag of Words (BoW) - Bolsa de Palabras\n",
        "\n",
        "* La Bolsa de Palabras (***BoW***) es un modelo que se utiliza para simplificar el contenido de un documento (o conjunto de documentos) omitiendo la gramática y el orden de las palabras, centrándose solo en el número de ocurrencias de palabras dentro del texto (o corpus).\n",
        "\n",
        "\n",
        "* A continuación se muestra con código python como obtendríamos una bolsa de palabras a partir del siguiente \"corpus\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPKKC8A-E4lL",
        "outputId": "a5dc6a13-8acc-42de-c84e-3516ef8faa3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'balon': 5, 'futbol': 15, 'liga': 4, 'ronaldo': 7, 'messi': 7, 'politica': 12, 'pp': 9, 'rajoy': 9, 'psoe': 9, 'zapatero': 9, '': 2, 'dinero': 9, 'fmi': 13, 'ue': 11, 'pib': 5, 'ibex': 2}\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "documents = [\"balon balon balon futbol futbol liga liga liga ronaldo ronaldo ronaldo messi\",\n",
        "            \"futbol futbol futbol futbol futbol ronaldo ronaldo ronaldo ronaldo messi messi\",\n",
        "            \"balon balon futbol futbol futbol futbol futbol futbol messi messi messi messi\",\n",
        "            \"politica politica politica pp pp pp pp pp pp rajoy rajoy rajoy rajoy rajoy\",\n",
        "            \"politica politica politica politica pp pp psoe psoe psoe psoe zapatero zapatero rajoy\",\n",
        "            \"politica politica politica psoe psoe psoe psoe zapatero zapatero zapatero zapatero \",\n",
        "            \"dinero fmi fmi fmi fmi fmi ue ue ue ue pib pib pib ibex ibex\",\n",
        "            \"zapatero rajoy dinero dinero dinero dinero fmi fmi fmi fmi ue ue ue ue pib\",\n",
        "            \"pp psoe zapatero rajoy dinero dinero dinero dinero fmi fmi fmi fmi ue ue ue \",\n",
        "            \"futbol politica pib\",\n",
        "            \"futbol liga politica zapatero rajoy\"]\n",
        "\n",
        "# Construimos la bolsa de palabras\n",
        "bow = dict()\n",
        "for doc in documents:\n",
        "    doc = doc.split(' ')\n",
        "    for word in doc:\n",
        "        if word in bow:\n",
        "            bow[word] += 1\n",
        "        else:\n",
        "            bow[word] = 1\n",
        "print(bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "uN-R0Y-tE4lM"
      },
      "source": [
        "* La construcción de las bolsas de palabras no solo se centran en la frecuencia si no que existen otras maneras de asignar un \"peso\" o \"importancia\" dentro del documento o del corpus a las palabras.\n",
        "\n",
        "\n",
        "* Aunque existen más formas de construir bolsas de palabras, las más utilizadas son las siguiente:\n",
        "\n",
        "    1. Vectores de Frecuencias\n",
        "    2. One-Hot-Encode\n",
        "    3. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "    \n",
        "    \n",
        "* Veamos a continuación como implementar estas bolsas de palabras con las siguientes librerías:\n",
        "\n",
        "    * ***scikit***\n",
        "    * ***NLTK***\n",
        "    * ***Gensim***\n",
        "    \n",
        "    \n",
        "<hr>\n",
        "\n",
        "\n",
        "# 1.- Vectores de Frecuencias\n",
        "\n",
        "* Los ***vectores de frecuencias*** es el método más trivial de construir las ***Bolsas de Palabras***.\n",
        "\n",
        "\n",
        "* Simplemente consiste en contar cuantas veces aparece una palabra en el documento del corpus.\n",
        "\n",
        "\n",
        "### -Scikit\n",
        "\n",
        "* Esta librería devuelve una matriz en la que las **filas representan a cada documento del corpus** y las **columnas el número de apariciones de las palabras**.\n",
        "\n",
        "\n",
        "* Para saber que palabra corresponde a cada columan de la matriz, scikit nos devuelve una lista en la que coinciden los indice de cada una de las palabras de la lista con la matriz.\n",
        "\n",
        "\n",
        "* Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qPBZHIBE4lM",
        "outputId": "e7900aad-268d-4c8b-d950-f70538a3ff68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['balon' 'dinero' 'fmi' 'futbol' 'ibex' 'liga' 'messi' 'pib' 'politica'\n",
            " 'pp' 'psoe' 'rajoy' 'ronaldo' 'ue' 'zapatero']\n",
            "[[3 0 0 2 0 3 1 0 0 0 0 0 3 0 0]\n",
            " [0 0 0 5 0 0 2 0 0 0 0 0 4 0 0]\n",
            " [2 0 0 6 0 0 4 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 3 6 0 5 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 4 2 4 1 0 0 2]\n",
            " [0 0 0 0 0 0 0 0 3 0 4 0 0 0 4]\n",
            " [0 1 5 0 2 0 0 3 0 0 0 0 0 4 0]\n",
            " [0 4 4 0 0 0 0 1 0 0 0 1 0 4 1]\n",
            " [0 4 4 0 0 0 0 0 0 1 1 1 0 3 1]\n",
            " [0 0 0 1 0 0 0 1 1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 1 0 0 1 0 0 1 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Resultados\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(vectors.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "KZfrLYwnE4lM"
      },
      "source": [
        "### -NLTK\n",
        "\n",
        "* NLTK trabajar con diccionarios (un diccionario por documento) cuyas claves son las palabras y los valores son el número de apariciones de esa palabra en el documento."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0J79zSEFKKZ",
        "outputId": "80ad8bd9-21ae-4ad1-b1b7-89b38db2e8ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4La7RWWIE4lM",
        "outputId": "8bc87d49-593a-4525-acf4-95ef7fe333a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'balon': 3, 'futbol': 2, 'liga': 3, 'ronaldo': 3, 'messi': 1})\n",
            "defaultdict(<class 'int'>, {'futbol': 5, 'ronaldo': 4, 'messi': 2})\n",
            "defaultdict(<class 'int'>, {'balon': 2, 'futbol': 6, 'messi': 4})\n",
            "defaultdict(<class 'int'>, {'politica': 3, 'pp': 6, 'rajoy': 5})\n",
            "defaultdict(<class 'int'>, {'politica': 4, 'pp': 2, 'psoe': 4, 'zapatero': 2, 'rajoy': 1})\n",
            "defaultdict(<class 'int'>, {'politica': 3, 'psoe': 4, 'zapatero': 4})\n",
            "defaultdict(<class 'int'>, {'dinero': 1, 'fmi': 5, 'ue': 4, 'pib': 3, 'ibex': 2})\n",
            "defaultdict(<class 'int'>, {'zapatero': 1, 'rajoy': 1, 'dinero': 4, 'fmi': 4, 'ue': 4, 'pib': 1})\n",
            "defaultdict(<class 'int'>, {'pp': 1, 'psoe': 1, 'zapatero': 1, 'rajoy': 1, 'dinero': 4, 'fmi': 4, 'ue': 3})\n",
            "defaultdict(<class 'int'>, {'futbol': 1, 'politica': 1, 'pib': 1})\n",
            "defaultdict(<class 'int'>, {'futbol': 1, 'liga': 1, 'politica': 1, 'zapatero': 1, 'rajoy': 1})\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(text):\n",
        "    for token in nltk.word_tokenize(text):\n",
        "        yield token\n",
        "\n",
        "def vectorize(corpus):\n",
        "    features = defaultdict(int)\n",
        "    for token in tokenize(corpus):\n",
        "        features[token] += 1\n",
        "    return features\n",
        "\n",
        "vectors = map(vectorize, documents)\n",
        "\n",
        "# Resultados\n",
        "for v in vectors:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "A7LyMmEOE4lM"
      },
      "source": [
        "### -Gensim\n",
        "\n",
        "* ***Gensim trabaja con \"Diccionarios\"*** (gensim.corpora.Dictionary) que es una estructura de datos en la que guarda el orden de las palabras que hay en el corpus.\n",
        "\n",
        "\n",
        "* Posteriormente se construye la Bolsa de Palabras con las frecuencias con la función \"***doc2bow***\"\n",
        "\n",
        "\n",
        "* Como resultado devuelve una lista por cada documento en la que se indicida por cada palabra del documento identificada por el 'id' del diccionario previamente construido la frecuencia de esa palabra en el documento.\n",
        "\n",
        "\n",
        "* Para más información ver el siguiente enlace: https://radimrehurek.com/gensim/corpora/dictionary.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q9vyWFWE4lM",
        "outputId": "6d5bccc3-3685-405d-82aa-4aa15d2cf9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diccionario de palabras -> palabra: frecuencia\n",
            "\n",
            "{'balon': 0, 'futbol': 1, 'liga': 2, 'messi': 3, 'ronaldo': 4, 'politica': 5, 'pp': 6, 'rajoy': 7, 'psoe': 8, 'zapatero': 9, 'dinero': 10, 'fmi': 11, 'ibex': 12, 'pib': 13, 'ue': 14}\n",
            "\n",
            "Apariciones de las palabras en los documentos:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 3), (1, 2), (2, 3), (3, 1), (4, 3)],\n",
              " [(1, 5), (3, 2), (4, 4)],\n",
              " [(0, 2), (1, 6), (3, 4)],\n",
              " [(5, 3), (6, 6), (7, 5)],\n",
              " [(5, 4), (6, 2), (7, 1), (8, 4), (9, 2)],\n",
              " [(5, 3), (8, 4), (9, 4)],\n",
              " [(10, 1), (11, 5), (12, 2), (13, 3), (14, 4)],\n",
              " [(7, 1), (9, 1), (10, 4), (11, 4), (13, 1), (14, 4)],\n",
              " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 4), (11, 4), (14, 3)],\n",
              " [(1, 1), (5, 1), (13, 1)],\n",
              " [(1, 1), (2, 1), (5, 1), (7, 1), (9, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import gensim\n",
        "\n",
        "tokenize = [nltk.word_tokenize(text) for text in documents]\n",
        "dictionary = gensim.corpora.Dictionary(tokenize)\n",
        "vectors = [dictionary.doc2bow(token) for token in tokenize]\n",
        "\n",
        "# Resultados\n",
        "print('Diccionario de palabras -> palabra: frecuencia\\n')\n",
        "print(dictionary.token2id)\n",
        "print('\\nApariciones de las palabras en los documentos:')\n",
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "lALqba_zE4lN"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "# 2.- One-Hot-Encode\n",
        "\n",
        "* Este método de construcción de la ***Bolsa de Palabras*** consiste en indicar con un flag ([0,1], [True, False], etc.) si una palabra aparece o no en el documento.\n",
        "\n",
        "\n",
        "### -Scikit\n",
        "\n",
        "* Las estructuras de datos de salida de ***Scikit*** son iguales de en el caso de la construcción del vector de frecuencias salvo que en el contenido de la matriz solo hay ceros y unos.\n",
        "\n",
        "\n",
        "* Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9CuzAKcE4lN",
        "outputId": "e52b0f66-9353-4d9e-a63f-92d6ea3db7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['balon' 'dinero' 'fmi' 'futbol' 'ibex' 'liga' 'messi' 'pib' 'politica'\n",
            " 'pp' 'psoe' 'rajoy' 'ronaldo' 'ue' 'zapatero']\n",
            "[[1 0 0 1 0 1 1 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 1 0 0 0 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 1 1 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 1 0 0 0 1]\n",
            " [0 1 1 0 1 0 0 1 0 0 0 0 0 1 0]\n",
            " [0 1 1 0 0 0 0 1 0 0 0 1 0 1 1]\n",
            " [0 1 1 0 0 0 0 0 0 1 1 1 0 1 1]\n",
            " [0 0 0 1 0 0 0 1 1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 1 0 0 1 0 0 1 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = vectorizer.fit_transform(documents)\n",
        "\n",
        "onehot = Binarizer()\n",
        "corpus = onehot.fit_transform(corpus.toarray())\n",
        "\n",
        "# Resultados\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "id": "Fy7wopG1E4lN"
      },
      "source": [
        "### -NLTK\n",
        "\n",
        "* La estructura de datos de salida de ***NLTK*** es la misma que en el caso de la construcción del vector de frecuencias salvo que los valores del diccionario son ***booleanos*** en vez de numéricos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbSsjbmTE4lN",
        "outputId": "3ac10132-190b-41c2-eed0-4ad7b7f80d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'balon': True, 'futbol': True, 'liga': True, 'ronaldo': True, 'messi': True}\n",
            "{'futbol': True, 'ronaldo': True, 'messi': True}\n",
            "{'balon': True, 'futbol': True, 'messi': True}\n",
            "{'politica': True, 'pp': True, 'rajoy': True}\n",
            "{'politica': True, 'pp': True, 'psoe': True, 'zapatero': True, 'rajoy': True}\n",
            "{'politica': True, 'psoe': True, 'zapatero': True}\n",
            "{'dinero': True, 'fmi': True, 'ue': True, 'pib': True, 'ibex': True}\n",
            "{'zapatero': True, 'rajoy': True, 'dinero': True, 'fmi': True, 'ue': True, 'pib': True}\n",
            "{'pp': True, 'psoe': True, 'zapatero': True, 'rajoy': True, 'dinero': True, 'fmi': True, 'ue': True}\n",
            "{'futbol': True, 'politica': True, 'pib': True}\n",
            "{'futbol': True, 'liga': True, 'politica': True, 'zapatero': True, 'rajoy': True}\n"
          ]
        }
      ],
      "source": [
        "def tokenize(text):\n",
        "    for token in nltk.word_tokenize(text):\n",
        "        yield token\n",
        "\n",
        "def vectorize(corpus):\n",
        "    return {token: True for token in tokenize(corpus)}\n",
        "\n",
        "vectors = map(vectorize, documents)\n",
        "\n",
        "# Resultados\n",
        "for v in vectors:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "id": "qwmbcIoUE4lN"
      },
      "source": [
        "### -Gensim\n",
        "\n",
        "* La estructura de datos de salida de ***Gensim*** es la misma que en el caso de la construcción de la Bolsa de Palabras de frecuencias salvo que los valores de las palabras en los documentos solo tendrán valor '1'.\n",
        "\n",
        "\n",
        "* Para más información ver el siguiente enlace: https://radimrehurek.com/gensim/corpora/dictionary.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOje6WXEE4lN",
        "outputId": "a8bc1ba6-0019-4178-f9e1-324dd92c86d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diccionario de palabras -> palabra: id\n",
            "\n",
            "{'balon': 0, 'futbol': 1, 'liga': 2, 'messi': 3, 'ronaldo': 4, 'politica': 5, 'pp': 6, 'rajoy': 7, 'psoe': 8, 'zapatero': 9, 'dinero': 10, 'fmi': 11, 'ibex': 12, 'pib': 13, 'ue': 14}\n",
            "\n",
            "Apariciones de las palabras en los documentos (id, 1):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
              " [(1, 1), (3, 1), (4, 1)],\n",
              " [(0, 1), (1, 1), (3, 1)],\n",
              " [(5, 1), (6, 1), (7, 1)],\n",
              " [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
              " [(5, 1), (8, 1), (9, 1)],\n",
              " [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1)],\n",
              " [(7, 1), (9, 1), (10, 1), (11, 1), (13, 1), (14, 1)],\n",
              " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (14, 1)],\n",
              " [(1, 1), (5, 1), (13, 1)],\n",
              " [(1, 1), (2, 1), (5, 1), (7, 1), (9, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tokenize = [nltk.word_tokenize(text) for text in documents]\n",
        "dictionary = gensim.corpora.Dictionary(tokenize)\n",
        "vectors = [[(token[0], 1) for token in dictionary.doc2bow(doc)] for doc in tokenize]\n",
        "\n",
        "# Resultados\n",
        "print('Diccionario de palabras -> palabra: id\\n')\n",
        "print(dictionary.token2id)\n",
        "print('\\nApariciones de las palabras en los documentos (id, 1):')\n",
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "id": "yr1RLBCgE4lN"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "# 3.- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "\n",
        "* El TF-IDF (Frecuencia de Termino - Frecuencia Inversa de Documento) es una medida numérica que permite expresar como de relevante es una palabra para un documento en una colección de documentos (o corpus).\n",
        "\n",
        "\n",
        "* Construir la Bolsa de Palabras con TF-IDF en vez de con frecuencias evita dar \"importancia\" a texto muy largos y con mucha repetición de palabras, frente a textos cortos y con pocas repeticiones de palabras.\n",
        "\n",
        "\n",
        "* La media de ***TF-IDF*** tiene dos componentes que son:\n",
        "    \n",
        "    * ***TF*** (Term Frecuency): Es la frecuencia con la que aparece la palabra en un documento del corpus. Esta se define como:\n",
        "    \n",
        "    $$tf(t,d) = 1 + log(f_{t,d})$$\n",
        "    \n",
        "    * ***IDF*** (Inverse Document Frequency): La frecuencia inversa del documento nos indica lo común que es una palabra en el corpus.\n",
        "    \n",
        "    $$idf(t,D) = log(1 + \\frac{N}{n_t})$$\n",
        "    \n",
        "\n",
        "* ***TF-IDF*** queda definido como:\n",
        "$$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D)$$\n",
        "\n",
        "\n",
        "\n",
        "### Ejemplos:\n",
        "\n",
        "* Veamos un ejemplo dado el siguiente corpus de 2 documentos con las siguientes palabras:\n",
        "\n",
        "```\n",
        "corpus = [\"messi messi messi ronaldo ronaldo balon\",\n",
        "          \"messi ronaldo futbol futbol futbol\"]\n",
        "```\n",
        "\n",
        "* ***Ejemplo 1***: Calculamos el ***tf-idf*** de la palabra \"**messi**\" para el documento 1:\n",
        "    \n",
        "    * ***TF***:\n",
        "        - t: número de veces que aparece la palabra \"messi\" en el documento 1 -> 3\n",
        "        - d: número de palabras que tiene el documento 1 -> 6\n",
        "        $$tf(t,d) = 1 + log(\\frac{3}{6}) =  0,69$$\n",
        "        \n",
        "    * ***IDF***:\n",
        "        - n<sub>t</sub>: número de documentos en los que aparece la palabra 'messi' -> 2\n",
        "        - D: número total de documentos en el corpus -> 2\n",
        "        $$idf(t,D) = log(1 + \\frac{2}{2}) = 0,3$$\n",
        "        \n",
        "    * ***TF-IDF***:\n",
        "    $$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D) = 0,69 * 0,3 = 0,21$$\n",
        "    \n",
        "    \n",
        "* ***Ejemplo 2***: Calculamos el ***tf-idf*** de la palabra \"**futbol**\" para el documento 2:\n",
        "    \n",
        "    * ***TF***:\n",
        "        - t: número de veces que aparece la palabra \"futbol\" en el documento 2 -> 3\n",
        "        - d: número de palabras que tiene el documento 1 -> 5\n",
        "        $$tf(t,d) = 1 + log(\\frac{3}{5}) =  0,78$$\n",
        "        \n",
        "    * ***IDF***:\n",
        "        - n<sub>t</sub>: número de documentos en los que aparece la palabra 'futbol' -> 1\n",
        "        - D: número total de documentos en el corpus -> 2\n",
        "        $$idf(t,D) = log(1 + \\frac{2}{1}) = 0,48$$\n",
        "        \n",
        "    * ***TF-IDF***:\n",
        "    $$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D) = 0,78 * 0,48 = 0,37$$\n",
        "        \n",
        "\n",
        "## Implementación:\n",
        "\n",
        "\n",
        "* El ejemplo mostrado anteriormente se ha realizado sobre el un TF-IDF teórico, calculando la frecuencia escalada logaritmicamente y sobre un corpus de \"juguete\" para entender el concepto.\n",
        "\n",
        "\n",
        "* Las implementaciones del ***TF-IDF*** de **Scikit** y **Gensim** estan pensadas para corpus con un número relevante de documentos y de palabras, por tanto la implementación del ***TF-IDF*** acepta una serie de parámetros para no tener en cuenta Stop Words, palabras irrelevantes, etc. por lo que si se realiza una implementación del ***TF-IDF*** según la bibliografia no van a conincidir los resultados de esa implementación con los resultados de las librerías de **Scikit** y **Gensim** a no ser que se modifiquen los parámetros de las funciones del ***TF-IDF***.\n",
        "\n",
        "\n",
        "### -Scikit\n",
        "\n",
        "* Las estructuras de datos de salida de ***Scikit*** son iguales de en el caso de la construcción del vector de frecuencias salvo que en el contenido de la matriz será de números decimales en vez de números enteros.\n",
        "\n",
        "\n",
        "* Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEm9qHO7E4lO",
        "outputId": "48769b0f-eca7-473b-ae6d-423992130478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['balon' 'dinero' 'fmi' 'futbol' 'ibex' 'liga' 'messi' 'pib' 'politica'\n",
            " 'pp' 'psoe' 'rajoy' 'ronaldo' 'ue' 'zapatero']\n",
            "[[0.54967598 0.         0.         0.26000769 0.         0.54967598\n",
            "  0.16113642 0.         0.         0.         0.         0.\n",
            "  0.54967598 0.         0.        ]\n",
            " [0.         0.         0.         0.63030611 0.         0.\n",
            "  0.31249927 0.         0.         0.         0.         0.\n",
            "  0.71067462 0.         0.        ]\n",
            " [0.34051088 0.         0.         0.72480795 0.         0.\n",
            "  0.59892051 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.31745291 0.78694939 0.         0.52908818\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.55616781 0.34467783 0.68935567 0.13904195\n",
            "  0.         0.         0.27808391]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.4260541  0.         0.70411076 0.\n",
            "  0.         0.         0.56807213]\n",
            " [0.         0.13121747 0.65608737 0.         0.34911415 0.\n",
            "  0.         0.39365242 0.         0.         0.         0.\n",
            "  0.         0.5248699  0.        ]\n",
            " [0.         0.5639857  0.5639857  0.         0.         0.\n",
            "  0.         0.14099642 0.         0.         0.         0.11375503\n",
            "  0.         0.5639857  0.11375503]\n",
            " [0.         0.60096496 0.60096496 0.         0.         0.\n",
            "  0.         0.         0.         0.15024124 0.15024124 0.12121369\n",
            "  0.         0.45072372 0.12121369]\n",
            " [0.         0.         0.         0.53177225 0.         0.\n",
            "  0.         0.659118   0.53177225 0.         0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.40871302 0.         0.57603355\n",
            "  0.         0.         0.40871302 0.         0.         0.40871302\n",
            "  0.         0.         0.40871302]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(documents)\n",
        "\n",
        "# Resultados\n",
        "print(tfidf.get_feature_names_out ())\n",
        "print(corpus.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "id": "VrJypVgEE4lO"
      },
      "source": [
        "### -Gensim\n",
        "\n",
        "* La estructura de datos de salida de ***Gensim*** es la misma que en el caso de la construcción de la Bolsa de Palabras de frecuencias salvo que los valores de las palabras en los documentos tendrán números decimales en vez de números enteros.\n",
        "\n",
        "\n",
        "* Para más información ver el siguiente enlace: https://radimrehurek.com/gensim/models/tfidfmodel.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp3Ou8IoE4lO",
        "outputId": "eb108460-ef97-48b0-d2e9-aa93c812265e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diccionario de palabras -> palabra: id\n",
            "\n",
            "{'balon': 0, 'futbol': 1, 'liga': 2, 'messi': 3, 'ronaldo': 4, 'politica': 5, 'pp': 6, 'rajoy': 7, 'psoe': 8, 'zapatero': 9, 'dinero': 10, 'fmi': 11, 'ibex': 12, 'pib': 13, 'ue': 14}\n",
            "\n",
            "Apariciones de las palabras en los documentos (id, tfidf):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 0.5625782665647677),\n",
              "  (1, 0.17346413313634462),\n",
              "  (2, 0.5625782665647677),\n",
              "  (3, 0.14292402346071878),\n",
              "  (4, 0.5625782665647677)],\n",
              " [(1, 0.4753096556596214), (3, 0.3133012785909152), (4, 0.8221453886448734)],\n",
              " [(0, 0.4364883596941802), (1, 0.6056363309296559), (3, 0.6653439309932482)],\n",
              " [(5, 0.2613558865696037), (6, 0.8613661899618252), (7, 0.43559314428267276)],\n",
              " [(5, 0.46092812545169537),\n",
              "  (6, 0.3797770814326364),\n",
              "  (7, 0.11523203136292384),\n",
              "  (8, 0.7595541628652728),\n",
              "  (9, 0.23046406272584768)],\n",
              " [(5, 0.3626105786520071), (8, 0.7967182135514425), (9, 0.4834807715360095)],\n",
              " [(10, 0.12439479522615843),\n",
              "  (11, 0.6219739761307922),\n",
              "  (12, 0.4591543106110927),\n",
              "  (13, 0.3731843856784753),\n",
              "  (14, 0.4975791809046337)],\n",
              " [(7, 0.08604721233664431),\n",
              "  (9, 0.08604721233664431),\n",
              "  (10, 0.5671818639333573),\n",
              "  (11, 0.5671818639333573),\n",
              "  (13, 0.14179546598333934),\n",
              "  (14, 0.5671818639333573)],\n",
              " [(6, 0.1512091023340141),\n",
              "  (7, 0.09175978685592859),\n",
              "  (8, 0.1512091023340141),\n",
              "  (9, 0.09175978685592859),\n",
              "  (10, 0.6048364093360564),\n",
              "  (11, 0.6048364093360564),\n",
              "  (14, 0.45362730700204235)],\n",
              " [(1, 0.46050649452343395), (5, 0.46050649452343395), (13, 0.758859365761191)],\n",
              " [(1, 0.33952362879716663),\n",
              "  (2, 0.734094559272588),\n",
              "  (5, 0.33952362879716663),\n",
              "  (7, 0.33952362879716663),\n",
              "  (9, 0.33952362879716663)]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenize = [nltk.word_tokenize(text) for text in documents]\n",
        "dictionary = gensim.corpora.Dictionary(tokenize)\n",
        "tfidf = gensim.models.TfidfModel(dictionary=dictionary, normalize=True)\n",
        "vectors = [tfidf[dictionary.doc2bow(doc)] for doc in tokenize]\n",
        "\n",
        "# Resultados\n",
        "print('Diccionario de palabras -> palabra: id\\n')\n",
        "print(dictionary.token2id)\n",
        "print('\\nApariciones de las palabras en los documentos (id, tfidf):')\n",
        "vectors"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}