{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sc2VmNkzrFI"
      },
      "source": [
        "# Conceptos para el Procesamiento del Lenguaje Natural (NLP)\n",
        "\n",
        "\n",
        "* En este Notebook vamos a enumerar y definir algunos de los conceptos más importantes que se dan en el Procesamiento del Lenguaje Natural y a ver algunos ejemplos en código.\n",
        "\n",
        "\n",
        "* Mostremos a continuación la definición de estos conceptos:\n",
        "\n",
        "\n",
        "### 1.- Corpus:\n",
        "\n",
        "* Un ***Corpus*** (en Latín \"cuerpo\") en el NLP se refiere a una colección de textos como pueda ser un conjunto de artículos periodísticos, libros, críticas, tweets, etc.\n",
        "\n",
        "### 2.- Bag of Words (BoW):\n",
        "\n",
        "* ***BoW*** (Bolsa de palabras) es un modelo que se utiliza para simplificar el contenido de un documento (o conjunto de documentos) omitiendo la gramática y el orden de las palabras, centrándose solo en el número de ocurrencias de palabras dentro del texto.\n",
        "\n",
        "### 3.- Normalización:\n",
        "\n",
        "* La ***normalización*** es una tarea que tiene como objetivo poner todo el texto en igualdad de condiciones:\n",
        "\n",
        "    - Convertir todo el texto en mayúscula o minúsculas\n",
        "    - Eliminar, puntos, comas, comillas, etc.\n",
        "    - Convertir los números a su equivalente a palabras\n",
        "    - Quitar palabras que no aportan significado al texto (Stop-words)\n",
        "    - Etc.\n",
        "\n",
        "### 4.- Tokenización:\n",
        "\n",
        "* Es una tarea que divide las cadenas de texto del documento en piezas más pequeñas o tokens. En la fase de tokenización los documentos se dividen en oraciones y estas se \"tokenizan\" en palabras. Aunque la tokenización es el proceso de dividir grandes cadenas de texto en cadenas más pequeñas, se suele diferenciar la:\n",
        "<span></span><br><br>\n",
        "    - ***Segmentación***: Tarea de dividir grandes cadenas de texto en piezas más pequeñas como oraciones o párrafos.\n",
        "<span></span><br><br>\n",
        "    - ***Tokenización***: Tarea de dividir grandes cadenas de texto solo y exclusivamente en palabras.\n",
        "\n",
        "\n",
        "### 5.- Stemming:\n",
        "\n",
        "* ***Stemming*** es el proceso de eliminar los afijos (sufijos, prefijos, infijos, circunflejos) de una palabra para obtener un tallo de palabra.\n",
        "<span></span><br><br>\n",
        "     + *Ejemplo*: Conduciendo -> conducir\n",
        "\n",
        "\n",
        "### 6.- Lematización:\n",
        "\n",
        "\n",
        "* La ***lematización*** es el proceso lingüístico que sustituye una palabra con forma flexionada (plurales, femeninos, verbos conjugados, etc.) por su lema; es decir, por una palabra válida en el idioma.\n",
        "\n",
        "\n",
        "* Si lo queremos definir de otra manera es sustituir una palabra con forma flexionada por la palabra que encontraríamos en el diccionario.\n",
        "<span></span><br><br>\n",
        "    + *Ejemplo*: Coches -> Coche; Guapas -> Guapo\n",
        "\n",
        "\n",
        "### 7.- Stop Words:\n",
        "\n",
        "\n",
        "* Son palabras que no aportan nada al significado de las frases como las preposiciones, determinantes, etc.\n",
        "\n",
        "\n",
        "### 8.- Parts-of-speech (POS) Tagging:\n",
        "\n",
        "\n",
        "* Consiste en asignar una etiqueta de categoría a las partes tokenizadas de una oración. El etiquetado POS más popular sería identificar palabras como sustantivos, verbos, adjetivos, etc.\n",
        "\n",
        "\n",
        "* En la lengua castellana nos podemos encontrar 9 categorías de palabras:\n",
        "\n",
        "    - Artículo o determinante\n",
        "    - Sustantivo o nombre\n",
        "    - Pronombre\n",
        "    - Verbo\n",
        "    - Adjetivo\n",
        "    - Adverbio\n",
        "    - Preposición\n",
        "    - Conjunción\n",
        "    - Interjección\n",
        "\n",
        "\n",
        "### 9.- n-grammas:\n",
        "\n",
        "\n",
        "* Los ***n-gramas*** son otro modelo de representación para simplificar los contenidos de selección de texto.\n",
        "\n",
        "\n",
        "* A diferencia de la representación sin orden de una bolsa de palabras (bag of words), el modelado de n-gramas está interesado en preservar secuencias contiguas de N elementos de la selección de texto.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# Ejemplos con NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBzcYqMezrFJ"
      },
      "source": [
        "## 4.- Tokenización\n",
        "\n",
        "* Divide las cadenas de texto del documento en piezas más pequeñas o tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCbvTBJH0peU",
        "outputId": "67290360-7f04-4770-ea4b-c2a64917bc40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0oInE2dzrFK",
        "outputId": "e98c0cbd-f513-47ab-eaf6-7fdfa5899710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Un', 'radar', 'multa', 'a', 'Mariano', 'Rajoy', 'por', 'caminar', 'demasiado', 'rapido']\n"
          ]
        }
      ],
      "source": [
        "doc = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
        "words = nltk.word_tokenize(doc)\n",
        "print (words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K4QCkOQzrFL"
      },
      "source": [
        "## 5.- Stemming\n",
        "\n",
        "* Proceso de eliminar los afijos\n",
        "\n",
        "\n",
        "* Para realizar el Stemming con NLTK tenemos que seleccionar el \"Stemmer\" adecuado dependiendo del idioma.\n",
        "\n",
        "\n",
        "* En NLTK existen dos \"Stemmers\" que son los siguientes:\n",
        "    * PorterStemmer\n",
        "    * SnowballStemmer\n",
        "\n",
        "\n",
        "* Para más información sobre estos ver el siguiente enlace: http://www.nltk.org/howto/stem.html\n",
        "<span></span><br><br>\n",
        "     + *Ejemplo en Inglés* con el *PorterStemmer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVqES-0uzrFL",
        "outputId": "c52a0bc1-e5d2-475b-c08f-ecbd429a63dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "minimum\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stm = PorterStemmer()\n",
        "print (stm.stem('running'))\n",
        "print (stm.stem('minimum'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12HOyDvzzrFM"
      },
      "source": [
        "* Los Stemmers de NLTK para idiomas distintos al Ingles son relativamente malos ya que NLTK esta pensado para la lengua inglesa.\n",
        "    + *Ejemplo en Español* con el *SnowballStemmer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tof_uflTzrFM",
        "outputId": "c1e11253-f143-48f4-fd5d-b9a53ab639ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corr\n",
            "minim\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stm = SnowballStemmer('spanish') # Hay que indicarle explicitamente el idioma\n",
        "print (stm.stem('corriendo'))\n",
        "print (stm.stem('mínimo'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taTwubFYzrFM"
      },
      "source": [
        "## 6.- Lematización\n",
        "\n",
        "\n",
        "* Proceso lingüístico que sustituye una palabra con forma flexionada (plurales, femeninos, verbos conjugados, etc.) por su lema; es decir, por una palabra válida en el idioma.\n",
        "\n",
        "\n",
        "* La Lematización que hace NLTK solo es buena para la lengua inglesa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7elJgC-RzrFM",
        "outputId": "74e637f4-6b38-4349-a48f-4e9ebc381ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n",
            "perros\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "print (lemm.lemmatize('dogs'))\n",
        "print (lemm.lemmatize('perros'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZplROTIzrFM"
      },
      "source": [
        "## 7.- Stop words\n",
        "\n",
        "\n",
        "* Son las palabras que no aportan nada al significado de la frase.\n",
        "\n",
        "\n",
        "* NLTK tiene para una serie de idiomas un listado de Stop Words.\n",
        "\n",
        "\n",
        "* Para el Español dispone de un listado de stop words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXuOyIGgzrFM",
        "outputId": "f3bdf075-626a-4826-a60e-bb5f7cbb234d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'éramos', 'vuestra', 'e', 'hubieses', 'seríais', 'uno', 'fuéramos', 'estuvierais', 'fuese', 'tengas', 'hubiese', 'tuviera', 'hasta', 'tuvieran', 'antes', 'habéis', 'tenéis', 'estuvieran', 'fueses', 'seamos', 'estén', 'estaremos', 'teníais', 'unos', 'soy', 'tendremos', 'estoy', 'vosotras', 'estuvisteis', 'estuvieron', 'sintiendo', 'hayas', 'estas', 'tenidos', 'estuviste', 'mis', 'hubiesen', 'de', 'fueseis', 'ti', 'también', 'ha', 'estado', 'hayamos', 'suya', 'donde', 'sean', 'habíais', 'mi', 'estará', 'muchos', 'en', 'entre', 'serías', 'habidas', 'una', 'tiene', 'somos', 'estar', 'tuviese', 'nuestro', 'seas', 'mí', 'hubieras', 'tendrías', 'tuyo', 'esa', 'te', 'nuestra', 'habido', 'estarás', 'tuvisteis', 'que', 'tenga', 'estada', 'tengáis', 'sois', 'estuvimos', 'seríamos', 'estabas', 'seremos', 'sentidos', 'a', 'le', 'eso', 'tenías', 'mío', 'habrías', 'algunas', 'hubisteis', 'esté', 'tengamos', 'estés', 'habidos', 'quienes', 'hayan', 'serás', 'yo', 'tenían', 'habrían', 'sentido', 'no', 'tus', 'suyo', 'tendréis', 'me', 'hubieron', 'será', 'estaríais', 'estuve', 'tuvierais', 'el', 'sobre', 'habían', 'fue', 'tú', 'con', 'estarían', 'fuésemos', 'estaban', 'hayáis', 'habías', 'muy', 'nosotras', 'más', 'tendrás', 'serían', 'él', 'nada', 'todos', 'tened', 'su', 'todo', 'habiendo', 'tendré', 'tendrían', 'teniendo', 'eras', 'sea', 'eres', 'del', 'ellas', 'habréis', 'tuviste', 'estéis', 'estuviésemos', 'estados', 'hubieran', 'tengan', 'erais', 'está', 'estuviera', 'tuya', 'estarán', 'tendrá', 'tu', 'o', 'hemos', 'sería', 'sentidas', 'tuvieseis', 'vuestros', 'estarías', 'desde', 'tendríamos', 'haya', 'eran', 'tuyos', 'ella', 'los', 'pero', 'tuvimos', 'para', 'porque', 'estaré', 'estando', 'como', 'vuestro', 'fueras', 'tendrán', 'otro', 'estaríamos', 'estuvo', 'estás', 'las', 'hubiste', 'algo', 'tanto', 'era', 'tenidas', 'tuviésemos', 'estuviesen', 'hubiéramos', 'fuera', 'hubiera', 'esas', 'sentida', 'y', 'os', 'tuyas', 'estaréis', 'estáis', 'habríais', 'nosotros', 'habría', 'sus', 'algunos', 'tendría', 'sentid', 'estuvieseis', 'contra', 'están', 'míos', 'cual', 'un', 'estuvieras', 'habríamos', 'he', 'tuvo', 'seréis', 'hubieseis', 'fuimos', 'serán', 'habrá', 'esos', 'qué', 'fueran', 'ya', 'fui', 'vosotros', 'habré', 'mucho', 'estábamos', 'por', 'estaba', 'la', 'fuisteis', 'cuando', 'estaría', 'les', 'fuiste', 'tienes', 'durante', 'estuviéramos', 'tuvieras', 'tuviesen', 'mía', 'hubo', 'tuviéramos', 'tenido', 'seáis', 'al', 'nuestras', 'fuerais', 'estabais', 'han', 'tenía', 'hay', 'tuvieron', 'siente', 'otra', 'estad', 'hubierais', 'hubimos', 'suyas', 'tuve', 'estuvieses', 'nuestros', 'había', 'tienen', 'otros', 'estamos', 'sin', 'estemos', 'tuvieses', 'habida', 'tenemos', 'habrás', 'hubiésemos', 'son', 'ellos', 'tengo', 'es', 'seré', 'fueron', 'sí', 'mías', 'habremos', 'tendríais', 'otras', 'quien', 'estuviese', 'esto', 'ante', 'este', 'habrán', 'ese', 'estadas', 'poco', 'nos', 'habíamos', 'fuesen', 'se', 'suyos', 'has', 'teníamos', 'estos', 'ni', 'hube', 'vuestras', 'lo', 'esta', 'tenida'}\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(set(stopwords.words('spanish')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_DU9w9XzrFM"
      },
      "source": [
        "* Este listado de palabras se utiliza para eliminarlas de los textos.\n",
        "\n",
        "\n",
        "* Veamos a continuación como obtener las stop words de una frase tras su tokenización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfT0BjjHzrFM",
        "outputId": "9acaf1a5-be43-40da-dc8c-ce894190bb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n",
            "por\n"
          ]
        }
      ],
      "source": [
        "doc = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
        "words = nltk.word_tokenize(doc)\n",
        "for word in words:\n",
        "        if word in stopwords.words('spanish'):\n",
        "            print (word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6aTA8vXzrFN"
      },
      "source": [
        "## 8.- Part of Speech (PoS)\n",
        "\n",
        "\n",
        "* Consiste en asignar una etiqueta de categoría a las partes tokenizadas de una oración: sustantivos, verbos, adjetivos, etc.\n",
        "\n",
        "\n",
        "* El PoS de NLTK solo esta disponible para el ingles y tiene las siguientes categorias:\n",
        "\n",
        "|Tag|Meaning|\n",
        "|---|---|\n",
        "|ADJ|adjective|\n",
        "|ADP|adposition|\n",
        "|ADV|adverb|\n",
        "|CONJ|conjunction|\n",
        "|DET|determiner|\n",
        "|NOUN|noun|\n",
        "|NUM|numeral|\n",
        "|PRT|particle|\n",
        "|PRON|pronoun|\n",
        "|VERB|verb|\n",
        "|.|punctuation|\n",
        "|X|other|\n",
        "\n",
        "\n",
        "* Nota: La tabla anterior no significa que solo asigne esas categorias, si no que tiene esas categorias y luego las va desgranando; por ejemplo, los verbos o adjetivos pueden ser de diferentes tipos y les pondrá una etiqueta en función de ese tipo.\n",
        "\n",
        "\n",
        "* Veamos a continuación un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4RlzoC5zrFN",
        "outputId": "ae368537-7784-42cd-c7c2-5293e3cd3f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Is', 'VBZ'), ('marathon', 'JJ'), ('running', 'VBG'), ('bad', 'JJ'), ('for', 'IN'), ('you', 'PRP'), ('?', '.')]\n"
          ]
        }
      ],
      "source": [
        "doc = nltk.word_tokenize('Is marathon running bad for you?')\n",
        "print (nltk.pos_tag(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2seA3T8zrFN"
      },
      "source": [
        "### PoS en Español:\n",
        "\n",
        "\n",
        "* Para poder \"tagear\" correctamente las palabras en Español, tenemos que descargarnos un diccionario específico para esta lengua.\n",
        "\n",
        "\n",
        "* El grupo de Procesamiento de Lenguaje Natural de la Universidad de Stanford ha desarrollado un diccionario en castellano que nos pertime etiquetar las palabras.\n",
        "\n",
        "\n",
        "* En el siguiente enlace se puede ver su descripción: https://nlp.stanford.edu/software/spanish-faq.shtml\n",
        "\n",
        "\n",
        "* Para ello debemos de descargarnos el software especifico proporcionado por la universidad de Standford a través del siguiente enlace: https://nlp.stanford.edu/software/tagger.shtml\n",
        "\n",
        "\n",
        "* Una vez descargada la librería tenemos que:\n",
        "    1. Descomprimir el fichero\n",
        "    2. Obtener el jar: stanford-postagger-3.9.2.jar\n",
        "    3. Obtener el tagger spanish.tagger que se encuentra dentro de la carpeta models\n",
        "* Estos ficheros necesarios ya estan copiados dentro del proyecto en la carpeta 'libs'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P ./../data/ https://raw.githubusercontent.com/pbonillor/DATA_SCIENCE_CON_PYTHON/main/MODULO_V/stanford-postagger-3.9.2.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8qxg0MY22dc",
        "outputId": "a9b7baa6-1bd8-4558-d4ce-5c3a18941e3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-10 01:35:55--  https://raw.githubusercontent.com/pbonillor/DATA_SCIENCE_CON_PYTHON/main/MODULO_V/stanford-postagger-3.9.2.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3667733 (3.5M) [application/octet-stream]\n",
            "Saving to: ‘./../data/stanford-postagger-3.9.2.jar’\n",
            "\n",
            "stanford-postagger- 100%[===================>]   3.50M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-11-10 01:35:55 (42.0 MB/s) - ‘./../data/stanford-postagger-3.9.2.jar’ saved [3667733/3667733]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P ./../data/ https://raw.githubusercontent.com/pbonillor/DATA_SCIENCE_CON_PYTHON/main/MODULO_V/spanish.tagger"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAgiKRRL29rW",
        "outputId": "5660c0e9-891b-4142-8dc1-d54b951838d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-10 01:35:56--  https://raw.githubusercontent.com/pbonillor/DATA_SCIENCE_CON_PYTHON/main/MODULO_V/spanish.tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7937545 (7.6M) [application/octet-stream]\n",
            "Saving to: ‘./../data/spanish.tagger’\n",
            "\n",
            "spanish.tagger      100%[===================>]   7.57M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-11-10 01:35:56 (81.8 MB/s) - ‘./../data/spanish.tagger’ saved [7937545/7937545]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdgtBolwzrFN",
        "outputId": "11b93ccc-bf4b-4291-cd21-217201dc6c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Un', 'DI'), ('radar', 'NCS'), ('multa', 'NCS'), ('a', 'SP'), ('Mariano', 'NP'), ('Rajoy', 'NP'), ('por', 'SP'), ('caminar', 'VMN'), ('demasiado', 'RG'), ('rapido', 'AQ')]\n"
          ]
        }
      ],
      "source": [
        "from nltk.internals import find_jars_within_path\n",
        "from nltk.tag import StanfordPOSTagger\n",
        "\n",
        "jar = \"./../data/stanford-postagger-3.9.2.jar\"\n",
        "tagger_file = \"./../data/spanish.tagger\"\n",
        "\n",
        "tagger = StanfordPOSTagger(tagger_file, jar)\n",
        "\n",
        "doc = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
        "words = nltk.word_tokenize(doc)\n",
        "tags = tagger.tag(words)\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvEx7t7JzrFN"
      },
      "source": [
        "* En este caso el \"taggeo\" es diferente al de NLTK.\n",
        "\n",
        "\n",
        "* Si nos fijamos en la documentación:\n",
        "    - ('Un', 'di0000') -> Article (indefinite)\n",
        "    - ('radar', 'nc0s000') -> Common noun (singular)\n",
        "    - ('multa', 'nc0s000') -> Common noun (singular)\n",
        "    - ('a', 'sp000') -> Preposition\n",
        "    - ('Mariano', 'np00000') -> Proper noun\n",
        "    - ('Rajoy', 'np00000') -> Proper noun\n",
        "    - ('por', 'sp000') -> Preposition\n",
        "    - ('caminar', 'vmn0000') -> Verb (main, infinitive)\n",
        "    - ('demasiado', 'rg') -> Adverb (general)\n",
        "    - ('rapido', 'aq0000')  -> Adjective (descriptive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qpzrYMezrFN"
      },
      "source": [
        "## 9.- n-grams\n",
        "\n",
        "* Modelo de representación que selecciona secuencias contiguas de N elementos de la selección de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nrOVzAaMzrFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc6a68f-1206-4d54-f51c-d913ffa7d04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Un', 'radar', 'multa')\n",
            "('radar', 'multa', 'a')\n",
            "('multa', 'a', 'Mariano')\n",
            "('a', 'Mariano', 'Rajoy')\n",
            "('Mariano', 'Rajoy', 'por')\n",
            "('Rajoy', 'por', 'caminar')\n",
            "('por', 'caminar', 'demasiado')\n",
            "('caminar', 'demasiado', 'rapido')\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "doc = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
        "words = nltk.word_tokenize(doc)\n",
        "num_elementos = 3\n",
        "n_grams = ngrams(words, num_elementos)\n",
        "for grams in n_grams:\n",
        "    print (grams)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}