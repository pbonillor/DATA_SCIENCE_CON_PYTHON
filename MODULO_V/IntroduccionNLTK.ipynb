{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%"
        },
        "id": "ulTwcLRdwfXM"
      },
      "source": [
        "# Introducción a la librería NLTK\n",
        "\n",
        "\n",
        "* NLTK (https://www.nltk.org/) es una librería para Python, usada para el análisis y la manipulación del lenguaje natural.\n",
        "\n",
        "## 1.- Instalación y descargar de las bases de datos\n",
        "\n",
        "* NLTK utiliza una serie de bases de datos léxicas para la manipulación del lenguaje natural.\n",
        "\n",
        "* También dispone de una una serie de corpus (colección de textos) que nos podemos descargar.\n",
        "\n",
        "* Para descargarnos las bases de datos y los corpus realizaremos lo siguiente:\n",
        "    1. Importar la librería nltk\n",
        "    2. llamar a al método \"download\"<sup>(*)</sup>\n",
        "    3. Aparecerá una ventana emergente para seleccionar todo lo que NLTK nos permite descargar\n",
        "\n",
        "```\n",
        ">> nltk.download('all')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Zz3HsDwfXO",
        "outputId": "9977d66a-b4ca-4335-866b-3824711cb67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amYsztmbwfXP"
      },
      "source": [
        "## 2.- Corpus\n",
        "\n",
        "\n",
        "* Un ***Corpus*** (en Latín \"*Cuerpo*\") dentro del contexto del NLP se refiere a una colección de textos como puede ser un conjunto de artítulo periodísticos, libros, críticas, tweets, etc.\n",
        "\n",
        "\n",
        "* NLTK dispone de una serie de ***Corpus*** con los que poder trabajar y realizar pruebas.\n",
        "\n",
        "\n",
        "* Algunos de los ***corpus*** que pueden ser de interés didáctico son los siguientes:\n",
        "\n",
        "|Corpus|Content|\n",
        "|---|---|\n",
        "|Brown Corpus|15 genres, 1.15M words, tagged, categorized|\n",
        "|CESS Treebanks|1M words, tagged and parsed (Catalan, Spanish)|\n",
        "|Gutenberg (selections)|18 texts, 2M words|\n",
        "|Inaugural Address Corpus|U.S. Presidential Inaugural Addresses (1789–present)|\n",
        "|Movie Reviews|2k movie reviews with sentiment polarity classification|\n",
        "|Reuters Corpus|1.3M words, 10k news documents, categorized|\n",
        "|Stopwords Corpus|2,400 stopwords for 11 languages|\n",
        "|WordNet 3.0 (English)|145k synonym sets|\n",
        "\n",
        "\n",
        "* Para más información relativa a los corpus ir al siguiente enlace: http://www.nltk.org/howto/\n",
        "\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKZtbfTiwfXP"
      },
      "source": [
        "### 2.1.- Manejo de Corpus (funcionalidades)\n",
        "\n",
        "Dentro de NLTK podemos encontrarnos diferentes tipos de Corpus que podrían clasificarse en:\n",
        "\n",
        "* Textos planos: como el corpus de *Gutenberg*\n",
        "* Textos categorizados: como el corpus de *Bronwn* (15 generos)\n",
        "* Textos multicategóricos: como el corpus de *Reuters* (1 documentos, varias categorias)\n",
        "* Textos temporales: como el corpus *Inaugural Address Corpus*, discursos presidenciales a lo largo de la historia\n",
        "\n",
        "Para el manejo de estos corpus NLTK nos ofrece las siguientes funciones:\n",
        "\n",
        "|Example|Description|\n",
        "|---|---|\n",
        "|fileids()|the files of the corpus|\n",
        "|fileids([categories])|the files of the corpus corresponding to these categories|\n",
        "|categories()|the categories of the corpus|\n",
        "|categories([fileids])|the categories of the corpus corresponding to these files|\n",
        "|raw()|the raw content of the corpus|\n",
        "|raw(fileids=[f1,f2,f3])|the raw content of the specified files|\n",
        "|raw(categories=[c1,c2])|the raw content of the specified categories|\n",
        "|words()|the words of the whole corpus|\n",
        "|words(fileids=[f1,f2,f3])|the words of the specified fileids|\n",
        "|words(categories=[c1,c2])|the words of the specified categories|\n",
        "|sents()|the sentences of the whole corpus|\n",
        "|sents(fileids=[f1,f2,f3])|the sentences of the specified fileids|\n",
        "|sents(categories=[c1,c2])|the sentences of the specified categories|\n",
        "|abspath(fileid)|the location of the given file on disk|\n",
        "|encoding(fileid)|the encoding of the file (if known)|\n",
        "|open(fileid)|open a stream for reading the given corpus file|\n",
        "|root|if the path to the root of locally installed corpus|\n",
        "|readme()|the contents of the README file of the corpus|\n",
        "\n",
        "### 2.1.1.- Ejemplo con el corpus de Gutenberg\n",
        "\n",
        "**1. ¿Que ficheros componen el corpus?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buMOTOe9wfXP",
        "outputId": "8b991787-8f6e-457e-d560-1315cfeba451"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqpPUkTJwfXQ"
      },
      "source": [
        "**2. ¿Cual es el contenido del fichero \"blake-poems.txt\"?**\n",
        "\n",
        "(Por legibilidad mostramos solo los 300 primeros caracteres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "1W3GQHhZwfXQ",
        "outputId": "8e1621b2-ad65-4d9b-f994-23ee157c6186"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[Poems by William Blake 1789]\\n\\n \\nSONGS OF INNOCENCE AND OF EXPERIENCE\\nand THE BOOK of THEL\\n\\n\\n SONGS OF INNOCENCE\\n \\n \\n INTRODUCTION\\n \\n Piping down the valleys wild,\\n   Piping songs of pleasant glee,\\n On a cloud I saw a child,\\n   And he laughing said to me:\\n \\n \"Pipe a song about a Lamb!\"\\n   So I piped'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "contenido = gutenberg.raw(\"blake-poems.txt\")\n",
        "contenido[:300]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxnegQqUwfXQ"
      },
      "source": [
        "**3. De cada uno de los ficheros mostramos:**\n",
        "    - Número de caracteres\n",
        "    - Número de palabras\n",
        "    - Número de frases\n",
        "    - Número de palabras distintas que aparecen en el texto (primero las pasamos a minúsculas)\n",
        "    - Número de medio de caracteres por palabra\n",
        "    - Número medio de palabras por frase\n",
        "    - Diversidad léxica (número de palabras / palabras distintas del texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsWh5i0uwfXQ",
        "outputId": "19992656-7d29-4ea0-aba9-3a1ce1fe5649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "887071     192427     7752       7344       4          24         26         austen-emma.txt\n",
            "466292     98171      3747       5835       4          26         16         austen-persuasion.txt\n",
            "673022     141576     4999       6403       4          28         22         austen-sense.txt\n",
            "4332554    1010654    30103      12767      4          33         79         bible-kjv.txt\n",
            "38153      8354       438        1535       4          19         5          blake-poems.txt\n",
            "249439     55563      2863       3940       4          19         14         bryant-stories.txt\n",
            "84663      18963      1054       1559       4          17         12         burgess-busterbrown.txt\n",
            "144395     34110      1703       2636       4          20         12         carroll-alice.txt\n",
            "457450     96996      4779       8335       4          20         11         chesterton-ball.txt\n",
            "406629     86063      3806       7794       4          22         11         chesterton-brown.txt\n",
            "320525     69213      3742       6349       4          18         10         chesterton-thursday.txt\n",
            "935158     210663     10230      8447       4          20         24         edgeworth-parents.txt\n",
            "1242990    260819     10059      17231      4          25         15         melville-moby_dick.txt\n",
            "468220     96825      1851       9021       4          52         10         milton-paradise.txt\n",
            "112310     25833      2163       3032       4          11         8          shakespeare-caesar.txt\n",
            "162881     37360      3106       4716       4          12         7          shakespeare-hamlet.txt\n",
            "100351     23140      1907       3464       4          12         6          shakespeare-macbeth.txt\n",
            "711215     154883     4250       12452      4          36         12         whitman-leaves.txt\n"
          ]
        }
      ],
      "source": [
        "for file in gutenberg.fileids():\n",
        "    num_chars = len(gutenberg.raw(file))\n",
        "    num_words = len(gutenberg.words(file))\n",
        "    num_sents = len(gutenberg.sents(file))\n",
        "    num_words_distinct = len(set([w.lower() for w in gutenberg.words(file)]))\n",
        "    avg_chars_words = int(num_chars/num_words)\n",
        "    avg_words_sents = int(num_words/num_sents)\n",
        "    lexical_diversity = int(num_words/num_words_distinct)\n",
        "    print(\"{num_chars:<10} {num_words:<10} {num_sents:<10} {num_words_distinct:<10} {avg_chars_words:<10} {avg_words_sents:<10} {lexical_diversity:<10} {file:<10}\"\n",
        "          .format(num_chars=num_chars, num_words=num_words, num_sents=num_sents,\n",
        "                  num_words_distinct=num_words_distinct, avg_chars_words=avg_chars_words,\n",
        "                  avg_words_sents = avg_words_sents, lexical_diversity=lexical_diversity, file=file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe9FqCcawfXQ"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### 2.1.2.- Ejemplo con el corpus de Brown\n",
        "\n",
        "Este es un corpus que contiene una serie de textos categorizados (o tageados) con un tipos de genero\n",
        "\n",
        "**1. ¿Cuales son las categorias del corpus de Brown?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QseS5dQ-wfXQ",
        "outputId": "2f411104-0237-498e-d462-e00928fe21a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fda-S0-wfXQ"
      },
      "source": [
        "***2. ¿Qué ficheros componen la categoría de noticias (news)?*** (por legibilidad solo mostramos 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S83SGePVwfXR",
        "outputId": "2ca01be1-dd99-4b88-d507-0d04de033c9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ca01', 'ca02', 'ca03', 'ca04', 'ca05']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "brown.fileids(['news'])[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Od1jOt4wfXR"
      },
      "source": [
        "***3.¿Que categorias corresponden al fichero \"ca01\"?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdu6M1N9wfXR",
        "outputId": "accf2569-958b-49e7-d3ad-5e8e27129c33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['news']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "brown.categories(fileids=['ca01'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xthakBrwwfXR"
      },
      "source": [
        "***4.¿Que palabras corresponden a la categoria humor?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St77LnBrwfXR",
        "outputId": "dda5d054-736d-4d31-93dd-3157e9e8f0fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It', 'was', 'among', 'these', 'that', 'Hinkle', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "brown.words(categories='humor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRXLmauRwfXR"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## 3.- WordNet\n",
        "\n",
        "* ***WordNet*** es un diccionario semántico y jerárquico en Ingles compuesto por una 155k palabras y 117K sinónimos.\n",
        "\n",
        "\n",
        "* De forma jerarquica, esta estructurado de tal manera que hay una serie de palabras llamadas \"***unique beginers***\" o \"*root synsets*\" que son palabras que definen \"conceptos\" muy generales y a partir de esos conceptos generales engloban una serie de palabras pertenecientes a ese concepto. Veamos el siguiente ejemplo:\n",
        "\n",
        "* En este ejemplo vemos como un \"*camión*\" (truck) esta definido como un \"*vehiculo motorizado*\" (motor vehicle) y este a su vez esta definido por otra palabra de nivel conceptual superior, hasta llegar a un muy alto nivel de palabra que lo define como un \"*artefacto*\" (artefact).\n",
        "\n",
        "\n",
        "* Este seria (\"a grandes rasgos\") como está organizado este diccionario, de tal manera que permite obtener de una palabra cosas como:\n",
        "    * Sinónimos\n",
        "    * Antónimos\n",
        "    * Hipérnimos\n",
        "    * Hipónimos\n",
        "    * Merónimos\n",
        "    * Holónimos\n",
        "    * Etc.\n",
        "    \n",
        "\n",
        "Veamos a continuación un ejemplo con la palabra \"motorcar\" y como nos daría una lista de sinónimos (synset) de esa palabra con la función \"synsets()\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtccoHQBwfXR",
        "outputId": "73653bad-1662-4676-8f88-14a597f93f82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "wn.synsets('motorcar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUYnPZ-5wfXR"
      },
      "source": [
        "* En este caso nos devuelve una lista de sinónimos (synset), que serian los \"nodos\" del diccionario jerarquico a partir del cual se relaciona esa palabra. Para este ejemplo solo nos ha dado un sinónimo.\n",
        "\n",
        "* A partir del \"nodo\" 'car.n.01' vamos a:\n",
        "    * Obtener su definición\n",
        "    * Obtener los lemas de sus sinónimos (de la palabra car no de la palabra motorcar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7kpQLUGwfXR",
        "outputId": "e25a054a-8839-46d4-a36f-5055e1eace24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definición: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
            "Sinónimos: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n"
          ]
        }
      ],
      "source": [
        "definicion = wn.synset('car.n.01').definition()\n",
        "sinonimos = wn.synset('car.n.01').lemma_names()\n",
        "\n",
        "print('Definición: ' + definicion)\n",
        "print('Sinónimos: ' + str(sinonimos))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otb0RPojwfXS"
      },
      "source": [
        "### Relación semántica entre palabras\n",
        "\n",
        "* Otro tema interesante que tenemos con ***WordNet*** es que nos permite ver la relación semáncia o la similaridad que hay entre palabras veamos por ejemplo la similaridad entre las siguientes palabras:\n",
        "\n",
        "    * car\n",
        "    * truck\n",
        "    * dog\n",
        "\n",
        "* Primero obtenemos alguno de los \"nodos\" de la palabra (el primero)\n",
        "* Comparamos la similaridad \"nodo\" con \"nodo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZccoigRawfXS",
        "outputId": "61a9c398-d7a6-4f5f-92d4-8d3b91da0b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridad entre Coche y Camión: 0.3333333333333333\n",
            "Similaridad entre Coche y Perro: 0.07692307692307693\n"
          ]
        }
      ],
      "source": [
        "car = wn.synsets('car')[0]\n",
        "truck = wn.synsets('truck')[0]\n",
        "dog = wn.synsets('dog')[0]\n",
        "\n",
        "print('Similaridad entre Coche y Camión: ' + str(car.path_similarity(truck)))\n",
        "print('Similaridad entre Coche y Perro: ' + str(car.path_similarity(dog)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRP_h9ujwfXS"
      },
      "source": [
        "* Aunque no hay una similaridad directa entre estas 3 palabras si que se puede apreciar que hay mayor similaridad entre dos automóviles que entre un animal y un automóvil."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}